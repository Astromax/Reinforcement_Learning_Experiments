# -*- coding: utf-8 -*-
"""QNetworks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sAh2eUwS_1QfhLxDFhxzEWjeAqxpx7ll
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class OldQNetwork(nn.Module):
    def __init__(self, state_size: int, action_size: int, seed: int=37, fc1_units: int=64, fc2_units: int=64):
        """Initialize parameters and build model"""
        super(OldQNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        
        self.fc1 = nn.Linear(state_size, fc1_units)
        self.fc2 = nn.Linear(fc1_units, fc2_units)
        self.fc3 = nn.Linear(fc2_units, action_size)
        
    def forward(self, state):
        """Build a network that maps state -> action values."""
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


#Somewhat larger Q-network with 2 hidden layers
class BaseQNetwork(nn.Module):
    def __init__(self, state_size: int, action_size: int, seed: int=37, fc1_units: int=64, fc2_units: int=64, fc3_units: int=64):
        super(BaseQNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)

        self.fc1 = nn.Linear(state_size, fc1_units)
        self.fc2 = nn.Linear(fc1_units, fc2_units)
        self.fc3 = nn.Linear(fc2_units, fc3_units)
        self.fc4 = nn.Linear(fc3_units, action_size)

    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)


class DuelingQNetwork(nn.Module):
    """
    This is a dueling network, in which the output of the last hidden layer is split 
    in two, one of which estimates the state-value function, the other estimates the action-advantage function.
    Q(s,a) = A(s,a) + V(s)
    Here we are using Q(s,a) = V(s) + A(s,a) - mean(A), as this allows the two functions to be learned more properly
    """
    def __init__(self, state_size: int, action_size: int, seed: int=37, fc1_units: int=64, fc2_units: int=64, fc3_units: int=32):
        super(DuelingQNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        
        self.fc1 = nn.Linear(state_size, fc1_units)
        self.fc2 = nn.Linear(fc1_units, fc2_units)
        self.fc3 = nn.Linear(fc2_units, fc3_units)
        self.value = nn.Linear(fc3_units, 1)
        self.advantage = nn.Linear(fc3_units, action_size)

    def forward(self, state):
        x = state
        #Ensure state is the right type, sometimes it's a batch (training), other times it's
        #a single state (interaction)
        #if not isinstance(x, torch.tensor):
        #    x = torch.tensor(x, device=self.device, dtype=torch.float32)
        #    x = x.unsqueeze(0)

        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        a = self.advantage(x)

        v = self.value(x)
        v = v.expand_as(a)

        q = v + a - a.mean(1, keepdim=True).expand_as(a) 
        return q